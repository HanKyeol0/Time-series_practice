LSTM_AE:
  feature_num: 5
  rnn_type: 'LSTM'
  rnn_inp_size: 64
  rnn_hid_size: 128
  nlayers: 2
  dropout: 0.1
  res_connection: false
  return_hiddens: false

AnomalyTransformer:
  enc_in: 25 # PSM dataset has 25 features
  c_out: 25 # PSM dataset has 25 features
  d_model: 512 # Appendix G: d_model=512 was the best for PSM dataset
  dropout: 0.1
  output_attention: false
  n_heads: 8 # specified in the paper
  d_ff: 2048
  activation: gelu
  e_layers: 3 # Appendix G (and "Implementation details"): L=3 was the best for PSM dataset
  k: 3 # Specified in "Implementation details". Actually it's Î» in the paper.

GDformer:
  d_src:
  d_tgt:
  d_in:
  g_n_head:
  d_n_head:
  d_k:
  d_v:
  d_out:
  d_prd:
  nodes:
  enc_layers: 3
  dec_layers: 3
  max_diff_step: 3
  droprate: 0.1
  diff_type: Attention
  adj_type: D
  dynamic_adj_saved: false

#  python main.py --anormly_ratio 1 --num_epochs 3    --batch_size 256  --mode train --dataset PSM  --data_path dataset/PSM --input_c 25    --output_c 25